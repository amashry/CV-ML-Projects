# -*- coding: utf-8 -*-
"""Final_copy_of_VIO_Final_Project.ipynb

Automatically generated by Colaboratory.

# Load Data and Helper Code Files
"""

# Commented out IPython magic to ensure Python compatibility.
## Import necessary libraries here
import os
import random
import copy 
import cv2
import numpy as np
from scipy.io import loadmat
import matplotlib.pyplot as plt
from matplotlib import cm
from scipy.linalg import svd
from scipy.linalg import cholesky
import numpy as np
from scipy.linalg import svd, cholesky, inv
from scipy.optimize import least_squares
from mpl_toolkits.mplot3d import Axes3D



# read list of images form dir in sorted order 
import glob
image_dir = '/Oxford_dataset_reduced/images/'
file_list = sorted(glob.glob(image_dir+'*.png'))

"""# 1. Estimate Rotations and Translations Between Frames
Estimate the 3D motion (translation and rotation) between successive frames in the sequence
by performing the following steps. You will perform these steps 376 times, starting from the
first image and ending at the second to last image. Be sure to store the rotations and
translations for use later.

# 1.1 Compute Intrinsic Matrix
Extract the camera parameters using ReadCameraModel.py as follows:
fx, fy, cx, cy, , LUT = ReadCameraModel(‘./Oxford dataset reduced/model’).
Using fx,fy,cx, and cy defined above, compute the camera’s intrinsic matrix K.
"""

from ReadCameraModel import ReadCameraModel

fx, fy, cx, cy,_,LUT = ReadCameraModel("/Oxford_dataset_reduced/model") 

K = np.array([(fx, 0, cx),
              (0, fy, cy),
              (0, 0, 1)])
K_inv = np.linalg.inv(K)

"""# 1.2 Load and Demoasic Images 

The input images are in Bayer format from which you can recover the color images using
the demosaic function with GBRG alignment. That is, load in the Bayer pattern encoded
image img and convert it into a color image using:
img = cv2.imread(filename,flags=-1)
and
color image = cv2.cvtColor(img, cv2.COLOR BayerGR2BGR)
Optionally undistort the current frame and next frame using UndistortImage.py as follows:
undistorted image = UndistortImage(color image,LUT)
"""

from UndistortImage import UndistortImage 

def load_and_undistort_image(image_path, LUT):
    raw_img = cv2.imread(image_path, flags=-1)
    color_img = cv2.cvtColor(raw_img, cv2.COLOR_BayerGR2BGR)
    undistorted_img = UndistortImage(color_img, LUT)
    return undistorted_img

"""# 1.3  Keypoint Correspondences
Find point correspondences between successive frames using a keypoint algorithm of your
choice. You are welcome to use code from online, just be sure to cite your source.
Hint: Don’t keep bad matches.

# 1.4 Estimate Fundamental Matrix

Using the matched keypoints you just identified, estimate the fundamental matrix between
the two frames.
Hint: See cv2.findFundamentalMat
"""

# New implementation using Zhang's method to generate 8x8 grid 
# Use 8-point algorithm with RANSAC 

def find_keypoints_and_matches(img1, img2):
    # Using SIFT detector
    sift = cv2.SIFT_create()
    kp1, des1 = sift.detectAndCompute(img1, None)
    kp2, des2 = sift.detectAndCompute(img2, None)
    
    # BFMatcher (instead of Flann)
    bf = cv2.BFMatcher()
    matches = bf.knnMatch(des1, des2, k=2)

    # Lowe's ratio test
    good = []
    for m, n in matches:
        if m.distance < 0.5 * n.distance:
            good.append(m)

    # Extract location of good matches
    points1 = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 2)
    points2 = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 2)

    return points1, points2, good, kp1, kp2

def generate_zhang_grid(img, matches, keypoints, grid_size):
    img_h, img_w = img.shape[:2]
    cell_h, cell_w = img_h // grid_size, img_w // grid_size

    # Initialize grid
    grid = [[[] for _ in range(grid_size)] for _ in range(grid_size)]

    # Loop over matches and sort them into grid cells
    for match in matches:
        pt = keypoints[match.queryIdx].pt
        i, j = int(pt[1] // cell_h), int(pt[0] // cell_w)
        grid[i][j].append(match)

    return grid

def eight_point_algorithm(img, points1, points2, matches, kp1, kp2, grid_size=8, iterations=300, threshold=0.1):
    best_F = None
    best_inliers = 0

    # Generate Zhang's grid
    grid = generate_zhang_grid(img, matches, kp1, grid_size)

    for _ in range(iterations):
        selected_points1 = []
        selected_points2 = []

        # Randomly select a point from each grid cell
        for i in range(grid_size):
            for j in range(grid_size):
                if grid[i][j]:
                    idx = np.random.choice(len(grid[i][j]))
                    selected_points1.append(kp1[grid[i][j][idx].queryIdx].pt)
                    selected_points2.append(kp2[grid[i][j][idx].trainIdx].pt)

        selected_points1 = np.array(selected_points1)
        selected_points2 = np.array(selected_points2)

        # Normalization
        T1, normalized_points1 = normalize(selected_points1)
        T2, normalized_points2 = normalize(selected_points2)

        # Matrix A
        A = np.array([normalized_points1[:, 0]*normalized_points2[:, 0],
                      normalized_points1[:, 0]*normalized_points2[:, 1],
                      normalized_points1[:, 0],
                      normalized_points1[:, 1]*normalized_points2[:, 0],
                      normalized_points1[:, 1]*normalized_points2[:, 1],
                      normalized_points1[:, 1],
                      normalized_points2[:, 0],
                      normalized_points2[:, 1],
                      np.ones(normalized_points1.shape[0])]).T

        # Solve for F
        _, _, V = svd(A)
        F = V[-1].reshape(3, 3)

        # Enforce rank 2 condition
        U, S, V = svd(F)
        S[2] = 0
        F = np.dot(U, np.dot(np.diag(S), V))

        # Denormalize
        F = np.dot(T2.T, np.dot(F, T1))

        # Normalize F
        F /= np.linalg.norm(F)

        # Ensure positive scaling factor
        if F[2, 2] < 0:
            F = -F

        # Check for inliers
        inliers = 0
        for i in range(points1.shape[0]):
            x1 = np.append(points1[i], 1)
            x2 = np.append(points2[i], 1)
            error = np.abs(np.dot(x2.T, np.dot(F, x1)))
            if error < threshold:
                inliers += 1

        if inliers > best_inliers:
            best_inliers = inliers
            best_F = F

    return best_F, best_inliers

def normalize(points):
    mean = np.mean(points, axis=0)
    std = np.std(points)
    T = np.array([[std, 0, mean[0]], [0, std, mean[1]],[0, 0, 1]])
    normalized_points = np.dot(T, np.append(points, np.ones((points.shape[0], 1)), axis=1).T).T[:, :2]
    
    return T, normalized_points

# Load your images here to test 
# img1 = load_and_undistort_image(file_list[125], LUT)
# img2 = load_and_undistort_image(file_list[126], LUT) 

# points1, points2, matches, kp1, kp2 = find_keypoints_and_matches(img1, img2)
# F, inliers = eight_point_algorithm(img1, matches, kp1, kp2, grid_size=8)

# print("Fundamental Matrix:\n", F)
# print("Number of inliers:", inliers)

"""# 1.5 Recover Essential Matrix
Estimate the Essential Matrix E from the Fundamental Matrix F by accounting for the
calibration parameters
"""

def estimate_essential_matrix(F, K):
    E = np.dot(K.T, np.dot(F, K))

    # Enforce rank 2 constraint on E
    U, S, Vt = np.linalg.svd(E)
    S = np.diag([1, 1, 0])
    E = np.dot(U, np.dot(S, Vt))

    return E

def calculate_camera_pose(E):
    # Possible camera matrices
    W = np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]])

    # Decompose essential matrix
    U, _, Vt = np.linalg.svd(E)
    
    # Get rotation and translation possibilities
    R1 = np.dot(U, np.dot(W, Vt))
    R2 = np.dot(U, np.dot(W.T, Vt))
    t1 = U[:, 2]
    t2 = -U[:, 2]

    # Check determinant constraint
    if np.linalg.det(R1) < 0:
        R1 = -R1
        t1 = -t1

    if np.linalg.det(R2) < 0:
        R2 = -R2
        t2 = -t2

    return [(R1, t1), (R2, t2)]

def linear_triangulation(K, P0, P1, x1, x2):
    x1 = np.dot(np.linalg.inv(K), np.append(x1, 1))
    x2 = np.dot(np.linalg.inv(K), np.append(x2, 1))

    A = np.zeros((4,4))
    A[0,:] = x1[0] * P0[2,:] - P0[0,:]
    A[1,:] = x1[1] * P0[2,:] - P0[1,:]
    A[2,:] = x2[0] * P1[2,:] - P1[0,:]
    A[3,:] = x2[1] * P1[2,:] - P1[1,:]

    _, _, Vt = np.linalg.svd(A)
    X = Vt[-1,:4]

    return X/X[3]

"""# Reconstruct the Trajectory

In the previous section, you should have computed 376 distinct rotations Ri and translations
ti
. Starting from i = 0, these matrices and vectors tell you how to translate and rotate a
point in camera i’s coordinate system to map it into camera i + 1’s coordinate system.
Assuming that the first video frame started at the origin, compute and plot the positions
of the camera centers (for each frame) based on the rotation and translation parameters
between successive frames. My reconstruction, projected onto 2 dimensions, is illustrated in
Figure 1. Include a 3-D reconstruction of the trajectory as well.
Hint: If the 4 × 4 matrix T12 takes a point in camera 1’s coordinate system and puts it in
camera 2’s coordinate system then T
−1
1,2
takes a point in camera 2’s coordinate system and
places it in camera 1’s coordinate system. Similarly, if the matrix product T23T12 takes a
point in camera 1’s coordinate system and maps it to camera 3’s coordinate system, then
(T23T12)
−1 = T
−1
12 T
−1
23 takes a point in camera 3’s coordinate system and maps it to camera
1’s coordinate system.
"""

def visual_odometry(file_list, K, LUT):
    trajectory = [np.zeros((3, 1))]  # Initialize trajectory with 3x1 vector
    R_world = np.eye(3)  # Initialize cumulative rotation matrix in the world frame
    T_world = np.zeros((3, 1))  # Initialize cumulative translation vector in the world frame

    try:
        for i in range(1, len(file_list)):
            # Load images
            img1 = load_and_undistort_image(file_list[i-1], LUT)
            img2 = load_and_undistort_image(file_list[i], LUT)

            # Find keypoints and matches
            points1, points2, matches, kp1, kp2 = find_keypoints_and_matches(img1, img2)

            # Estimate fundamental matrix
            F, _ = eight_point_algorithm(img1, points1, points2, matches, kp1, kp2, grid_size=8)

            # Estimate essential matrix
            E = estimate_essential_matrix(F, K)

            # Get possible camera poses
            poses = calculate_camera_pose(E)

            # Find best pose
            best_pose = None
            best_count = 0
            for R, t in poses:
                count = 0
                P1 = np.hstack((R, t.reshape(3, 1)))
                P0 = np.hstack((np.eye(3), np.zeros((3, 1))))
                for point1, point2 in zip(points1, points2):
                    X = linear_triangulation(K, P0, P1, point1, point2)
                    if np.dot(R[2, :], (X[:3] - t)) > 0:
                        count += 1
                if count > best_count:
                    best_count = count
                    best_pose = (R, t)

            R_new, T_new = best_pose

            # Update cumulative rotation and translation in the world frame
            T_world = T_world + R_world @ T_new.reshape(3, 1)
            R_world = R_new @ R_world

            # Append the new camera position in the world frame to the trajectory
            trajectory.append(T_world)
            print(i)

    except Exception as e:
        print("Exception occurred:", e)

    return trajectory

# Main function
trajectory = visual_odometry(file_list, K, LUT)

# Convert trajectory points into array for easier manipulation
trajectory_array = np.array(trajectory).squeeze()

# Plotting
plt.figure(figsize=(5,5))
plt.plot(trajectory_array[:,0], trajectory_array[:,2], 'b-')
plt.title('Camera Trajectory')
plt.show()

# Plotting in 3D
fig = plt.figure(figsize=(7,7))
ax = fig.add_subplot(111, projection='3d')
ax.plot(trajectory_array[:,0], trajectory_array[:,1], trajectory_array[:,2])
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
plt.title('Camera Trajectory in 3D')
plt.show()