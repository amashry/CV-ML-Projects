# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

# Final Project: Visual Odometry - OpenCV Built-in

Visual odometry is an important concept in robotic perception where it is used to estimate
the trajectory of a robot (or more precisely the trajectory of a robot's camera). Visual
odometry is conceptually similar to structure from motion, but instead reconstructs motion
from motion.
In this project you are given frames of a driving sequence taken by a camera in a car and a
script to extract the camera's intrinsic parameters. Using this sequence of frames, you will
perform a series of steps (identify shared keypoints, estimate the fundamental and essential
matrices between frames, decompose the essential matrices into translation and rotation
parameters, plot the camera center) to reconstruct and visualize the 3D trajectory of the
camera

# Load Data and Helper Code Files
"""

# Commented out IPython magic to ensure Python compatibility.
## Import necessary libraries here
import os
import random
import cv2
import numpy as np
from scipy.io import loadmat
import matplotlib.pyplot as plt
from matplotlib import cm
from scipy.linalg import svd
from scipy.linalg import cholesky
import numpy as np
from scipy.linalg import svd, cholesky, inv
from scipy.optimize import least_squares
from mpl_toolkits.mplot3d import Axes3D



# read list of images form dir in sorted order 
import glob
image_dir = '/Oxford_dataset_reduced/images/'
file_list = sorted(glob.glob(image_dir+'*.png'))
file_list

"""# 1. Estimate Rotations and Translations Between Frames
Estimate the 3D motion (translation and rotation) between successive frames in the sequence
by performing the following steps. You will perform these steps 376 times, starting from the
first image and ending at the second to last image. Be sure to store the rotations and
translations for use later.

# 1.1 Compute Intrinsic Matrix
Extract the camera parameters using ReadCameraModel.py as follows:
fx, fy, cx, cy, , LUT = ReadCameraModel('./Oxford dataset reduced/model').
Using fx,fy,cx, and cy defined above, compute the camera's intrinsic matrix K.
"""

from ReadCameraModel import ReadCameraModel

fx, fy, cx, cy,_,LUT = ReadCameraModel("/Oxford_dataset_reduced/model") 

K = np.array([(fx, 0, cx),
              (0, fy, cy),
              (0, 0, 1)])
K

"""# 1.2 Load and Demoasic Images 

The input images are in Bayer format from which you can recover the color images using
the demosaic function with GBRG alignment. That is, load in the Bayer pattern encoded
image img and convert it into a color image using:
img = cv2.imread(filename,flags=-1)
and
color image = cv2.cvtColor(img, cv2.COLOR BayerGR2BGR)
Optionally undistort the current frame and next frame using UndistortImage.py as follows:
undistorted image = UndistortImage(color image,LUT)
"""

from UndistortImage import UndistortImage 

def load_and_undistort_image(image_path, LUT):
    raw_img = cv2.imread(image_path, flags=-1)
    color_img = cv2.cvtColor(raw_img, cv2.COLOR_BayerGR2BGR)
    undistorted_img = UndistortImage(color_img, LUT)
    return undistorted_img

"""# 1.3  Keypoint Correspondences
Find point correspondences between successive frames using a keypoint algorithm of your
choice. You are welcome to use code from online, just be sure to cite your source.
Hint: Don’t keep bad matches.

"""

def find_keypoints_and_matches(img1, img2):
    # Using SIFT detector
    sift = cv2.SIFT_create()
    kp1, des1 = sift.detectAndCompute(img1, None)
    kp2, des2 = sift.detectAndCompute(img2, None)
    
    # Flann based matcherx
    FLANN_INDEX_KDTREE = 1
    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
    search_params = dict(checks=50)
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matches = flann.knnMatch(des1, des2, k=2)

    # store all the good matches following Lowe's ratio test.
    good = []
    for m, n in matches:
        if m.distance < 0.5 * n.distance:
            good.append(m)

    # Extract location of good matches
    points1 = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
    points2 = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

    return points1, points2


############# To draw Matches: ######################
# img1 = load_and_undistort_image(file_list[120], LUT)
# img2 = load_and_undistort_image(file_list[121], LUT) 

# sift = cv2.SIFT_create()
# kp1, des1 = sift.detectAndCompute(img1, None)
# kp2, des2 = sift.detectAndCompute(img2, None)

# # Flann based matcher
# FLANN_INDEX_KDTREE = 0
# index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
# search_params = dict(checks=70)
# flann = cv2.FlannBasedMatcher(index_params, search_params)
# matches = flann.knnMatch(des1, des2, k=2)

# # store all the good matches following Lowe's ratio test.
# good = []
# for m, n in matches:
#     if m.distance < 0.5 * n.distance:
#         good.append(m)

# # Extract location of good matches
# points1 = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
# points2 = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

# matches_plot = cv2.drawMatches(cv2.cvtColor(img1,cv2.COLOR_BGR2RGB), kp1, cv2.cvtColor(img2,cv2.COLOR_BGR2RGB), kp2, good, None, flags=2)

# plt.figure(figsize=(15,10))
# plt.imshow(matches_plot)
# plt.axis('off')
# plt.show()
##########################################################
"""# 1.4 Estimate Fundamental Matrix

Using the matched keypoints you just identified, estimate the fundamental matrix between
the two frames.
Hint: See cv2.findFundamentalMat
"""

def estimate_fundamental_matrix(points1, points2):
    F, mask = cv2.findFundamentalMat(points1, points2, cv2.FM_RANSAC, 0.03)
    # select only inlier points
    points1 = points1[mask.ravel() == 1]
    points2 = points2[mask.ravel() == 1]

    # Draw epipolar lines
    # draw_epipolar_lines(img1, img2, points1, points2, F)

    return F, points1, points2

def draw_epipolar_lines(img1, img2, points1, points2, F):
    # Convert image color to RGB for visualization
    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)
    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)

    # Compute epilines for points in image 2 and draw them on image 1
    lines1 = cv2.computeCorrespondEpilines(points2.reshape(-1, 1, 2), 2, F)
    lines1 = lines1.reshape(-1, 3)
    img1_epilines = img1_rgb.copy()
    for r, pt in zip(lines1, points1):
        color = tuple(np.random.randint(0, 255, 3).tolist())
        x0, y0 = map(int, [0, -r[2]/r[1]])
        x1, y1 = map(int, [img1.shape[1], -(r[2]+r[0]*img1.shape[1])/r[1]])
        img1_epilines = cv2.line(img1_epilines, (x0, y0), (x1, y1), color, 1)
        img1_epilines = cv2.circle(img1_epilines, tuple(map(int, pt[0])), 5, color, -1)

    # Compute epilines for points in image 1 and draw them on image 2
    lines2 = cv2.computeCorrespondEpilines(points1.reshape(-1, 1, 2), 1, F)
    lines2 = lines2.reshape(-1, 3)
    img2_epilines = img2_rgb.copy()
    for r, pt in zip(lines2, points2):
        color = tuple(np.random.randint(0, 255, 3).tolist())
        x0, y0 = map(int, [0, -r[2]/r[1]])
        x1, y1 = map(int, [img2.shape[1], -(r[2]+r[0]*img2.shape[1])/r[1]])
        img2_epilines = cv2.line(img2_epilines, (x0, y0), (x1, y1), color, 1)
        img2_epilines = cv2.circle(img2_epilines, tuple(map(int, pt[0])), 5, color, -1)

    # Concatenate images and display
    img = np.concatenate((img1_epilines, img2_epilines), axis=1)
    plt.figure(figsize=(12,7))
    plt.imshow(img)
    plt.axis('off')
    plt.show()

# img1 = load_and_undistort_image(file_list[140], LUT)
# img2 = load_and_undistort_image(file_list[141], LUT) 

# points1, points2 = find_keypoints_and_matches(img1,img2)
# print(points1.shape)
# F, points1, points2 = estimate_fundamental_matrix(points1, points2)
# print(points1.shape)
# # Draw epipolar lines
# draw_epipolar_lines(img1, img2, points1, points2, F)

"""# 1.5 Recover Essential Matrix
Estimate the Essential Matrix E from the Fundamental Matrix F by accounting for the
calibration parameters
"""

def estimate_essential_matrix(points1, points2, K):

    E, mask = cv2.findEssentialMat(points1, points2, focal=fx, pp=(cx,cy), method=cv2.RANSAC, prob=0.999, threshold=0.3)
    points1 = points1[mask.ravel() == 1]
    points2 = points2[mask.ravel() == 1]
    return E, mask, points1, points2

"""# 1.6 Reconstruct Rotation and Translation Parameters from E 

Decompose E into a physically realizable translation T and rotation R. That is, among the
four possible decompositions, chose the one that satisfies the depth positivity constraint.
Hint: See cv2.recoverPose.
"""

def estimate_motion(img1, img2, K):
    # Find keypoints and matches
    points1, points2 = find_keypoints_and_matches(img1, img2)

    # Estimate fundamental matrix
    F, points1, points2 = estimate_fundamental_matrix(points1, points2)

    # Estimate essential matrix
    E, mask, points1, points2 = estimate_essential_matrix(points1, points2, K)

    # Recover pose from essential matrix
    _, R, t, mask = cv2.recoverPose(E, points1, points2, K)

    return R, t

"""######################## Reconstruct Rotation and Translation Parameters Yourself #################################
Write a function to replace the cv2.recoverPose function in the pipeline. Your function should first decompose the essential matrix into 4 distinct combinations of rotations and translations; each of these combinations represents a potential camera matrix from frame i+1. It should then triangulate all the points in the scene 4 times, once per camera matrix (assuming frame i’s camera used the world coordinate system). You should return the translation/rotation pair associated with the camera matrix for which most matched
points are in front of both cameras.
"""

# def recover_pose_from_essential_matrix(E, points1, points2, K):
#     # Decompose the essential matrix into R1, R2, and t
#     R1, R2, t = cv2.decomposeEssentialMat(E)

#     # Four possible combinations for rotation and translation matrices
#     combinations = [(R1, t), (R2, t), (R1, -t), (R2, -t)]

#     best_score = -np.inf
#     R_best, t_best = None, None

#     # For each combination, triangulate points and check if they're in front of both cameras
#     for (R, t) in combinations:
#         # Create projection matrices for each combination
#         P = K @ np.hstack((R, t.reshape(-1, 1)))  # use non-homogeneous form of (R|t)

#         points_3D_homog = cv2.triangulatePoints(np.eye(3, 4), P, points1, points2)
#         points_3D = points_3D_homog[:3, :] / points_3D_homog[3, :]
#         points_3D_transformed = R @ points_3D + t.reshape(-1, 1)

#         # Calculate score as the sum of points with positive z-coordinates in both camera systems
#         score = np.sum(points_3D[2, :] > 0) + np.sum(points_3D_transformed[2, :] > 0)

#         # If this score is the best we've seen so far, store this combination as the best
#         if score > best_score:
#             best_score = score
#             R_best, t_best = R, t

#     # If none of the combinations resulted in points in front of both cameras, return default R and t
#     if R_best is None or t_best is None:
#         R_best = np.eye(3)
#         t_best = np.zeros(3)

#     return R_best, t_best

# def estimate_motion(img1, img2, K):
#     # Find keypoints and matches
#     points1, points2 = find_keypoints_and_matches(img1, img2)

#     # Estimate fundamental matrix
#     F, points1, points2 = estimate_fundamental_matrix(points1, points2)

#     # Estimate essential matrix
#     E, mask, points1, points2 = estimate_essential_matrix(points1, points2, K)

#     # Recover pose from essential matrix
#     R, t = recover_pose_from_essential_matrix(E, points1, points2, K)

#     return R, t

# def decompose_essential_matrix(E):
#     """Decompose an essential matrix into the possible rotation and translation matrices"""
#     U, _, VT = np.linalg.svd(E)
#     W = np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]])

#     R1 = np.matmul(U, np.matmul(W, VT))
#     R2 = np.matmul(U, np.matmul(W, VT))
#     R3 = np.matmul(U, np.matmul(W.T, VT))
#     R4 = np.matmul(U, np.matmul(W.T, VT))
#     R_set = [R1, R2, R3, R4]


#     t1 = U[:, 2]
#     t2 = -U[:, 2]
#     t3 = U[:, 2]
#     t4 = -U[:, 2]
#     t_set = [t1, t2, t3, t4]

#     # Correct rotation matrices if necessary
#     for i in range(len(t_set)):
#       if np.linalg.det(R_set[i]) < 0:
#         R_set[i] = -R_set[i]
#         t_set[i] = -t_set[i]

#     return R_set, t_set 


# def triangulate_point(P1, P2, point1, point2):
#     """Triangulate a single point in 3D space given two camera views"""
#     A = np.array([
#         point1[0] * P1[2, :] - P1[0, :],
#         point1[1] * P1[2, :] - P1[1, :],
#         point2[0] * P2[2, :] - P2[0, :],
#         point2[1] * P2[2, :] - P2[1, :]
#     ])
#     _, _, V = np.linalg.svd(A)
#     X = V[-1, :4]
#     return X / X[3]


# def recover_pose(E, points1, points2, K):
#     """Recover the rotation and translation from the essential matrix"""
#     R_set, t_set = decompose_essential_matrix(E)
#     max_inliers = 0
#     R_max, t_max = None, None

#     for i in range(4):
#         R, t = R_set[i], t_set[i]

#         # Projection matrices
#         P1 = np.dot(K, np.hstack((np.eye(3), np.zeros((3, 1)))))
#         P2 = np.dot(K, np.hstack((R, t.reshape(3, 1))))

#         inliers = 0
#         for i in range(points1.shape[0]):
#             X = triangulate_point(P1, P2, np.append(points1[i], 1), np.append(points2[i], 1))

#             # Check if the point is in front of both cameras
#             if X[2] > 0 and np.dot(R[2, :], X[:3] - t) > 0:
#                 inliers += 1

#         if inliers > max_inliers:
#             max_inliers = inliers
#             R_max, t_max = R, t

#     return R_max, t_max

# def estimate_motion(img1, img2, K):
#     # Find keypoints and matches
#     points1, points2 = find_keypoints_and_matches(img1, img2)

#     # Estimate fundamental matrix
#     F, points1, points2 = estimate_fundamental_matrix(points1, points2)

#     # Estimate essential matrix
#     E, mask, points1, points2 = estimate_essential_matrix(points1, points2, K)

#     # Recover pose from essential matrix
#     R, t = recover_pose(E, points1, points2, K)

#     return R, t
#########################################################################################

"""# Reconstruct the Trajectory

In the previous section, you should have computed 376 distinct rotations Ri and translations
ti
. Starting from i = 0, these matrices and vectors tell you how to translate and rotate a
point in camera i’s coordinate system to map it into camera i + 1’s coordinate system.
Assuming that the first video frame started at the origin, compute and plot the positions
of the camera centers (for each frame) based on the rotation and translation parameters
between successive frames. My reconstruction, projected onto 2 dimensions, is illustrated in
Figure 1. Include a 3-D reconstruction of the trajectory as well.
Hint: If the 4 × 4 matrix T12 takes a point in camera 1’s coordinate system and puts it in
camera 2’s coordinate system then T
−1
1,2
takes a point in camera 2’s coordinate system and
places it in camera 1’s coordinate system. Similarly, if the matrix product T23T12 takes a
point in camera 1’s coordinate system and maps it to camera 3’s coordinate system, then
(T23T12)
−1 = T
−1
12 T
−1
23 takes a point in camera 3’s coordinate system and maps it to camera
1’s coordinate system.
"""

def visual_odometry(file_list, K, LUT):
    trajectory = [np.zeros((3, 1))]  # Initialize trajectory with 3x1 vector
    R_world = np.eye(3)  # Initialize cumulative rotation matrix in the world frame
    T_world = np.zeros((3, 1))  # Initialize cumulative translation vector in the world frame
    prev_img = load_and_undistort_image(file_list[0], LUT)  # Load first image

    for i in range(1, len(file_list)):
        curr_img = load_and_undistort_image(file_list[i], LUT)
        R, t = estimate_motion(prev_img, curr_img, K)
        # deter = np.linalg.det(R)
        # print("Frame "+str(i-1)+"  det(R) = "+str(deter)+" norm (t): "+str(np.linalg.norm(t)))

        # Update cumulative rotation and translation in the world frame
        T_world = T_world + R_world @ t
        R_world = R @ R_world

        # Append the new camera position in the world frame to the trajectory
        trajectory.append(T_world)

        prev_img = curr_img

    return trajectory

# Main function
trajectory = visual_odometry(file_list, K, LUT)

# Convert trajectory points into array for easier manipulation
trajectory_array = np.array(trajectory).squeeze()

# Plotting
plt.figure(figsize=(5,5))
plt.plot(trajectory_array[:,0], -trajectory_array[:,2], 'b-')
plt.title('Camera Trajectory')
plt.show()

# Plotting in 3D
fig = plt.figure(figsize=(7,7))
ax = fig.add_subplot(111, projection='3d')
ax.plot(trajectory_array[:,0], trajectory_array[:,1], trajectory_array[:,2])
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
plt.title('Camera Trajectory in 3D')
plt.show()

